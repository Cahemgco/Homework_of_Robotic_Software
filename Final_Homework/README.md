# 《基于示教学习的复杂场景导航》README


## 项目概述

本项目主要实现了一种基于示教学习的复杂场景导航技术，旨在通过模仿人
类行动路径，实现机器人在复杂场景下的自主导航。具体任务流程分为三个阶段：

- 建图阶段：使用建图算法，对场景地图进行建立。

- 示教阶段：机器人跟踪前方的示教者，示教者通过语音向机器人发出保存当
前点位坐标信息的指令，机器人存储当前点位坐标并通过语音回复示教者。

- 物品抓取阶段：机器人根据多个保存的点位坐标，模仿示教者的路径，自主
导航至终点位置。在终点位置，机器人根据视觉反馈进行 PID 位置调整，完
成目标物体的抓取。最后机器人原路返回至起点，将目标物体带回起点位置。

## 代码架构
code文件夹中包括了项目中所编写的主要功能代码。

```
code/
│
├── amcl_follower.launch      # 行人跟踪+机器人定位的launch文件
│
├── voice_assistant_play.cpp  # 科大讯飞语音识别代码
│
├── my_dynamixel/             # 机械臂控制包
│   ├── CMakeLists.txt
│   ├── package.xml
│   ├── src/                  # 项目实现的Python代码
│   │   ├── all.py            # 整合物品抓取阶段的所有过程
│   │   ├── control_arm.py    # 控制机械臂进行物品抓取
│   │   ├── store_pose.py     # 存储当前机器人坐标信息
│   │   ├── get_pose.py       # 获取存储的机器人坐标信息
│   │   ├── pid.py            # 根据YOLO识别的物体位置，利用PID控制调整抓取位置
│   ├── include/              # 适用于Ubuntu20.04的机械臂控制
│   ├── launch/               # 适用于Ubuntu20.04的机械臂控制
│   └── config/
│
└── README.md                 # 项目说明文件
```

## 主要文件和功能说明

### 1. amcl_follower.launch

`amcl_follower.launch` 文件用于启动机器人行人跟踪和定位功能。它集成了AMCL（自适应蒙特卡洛定位）算法和行人跟踪算法，实现机器人在动态环境中的定位和跟随功能。

### 2. voice_assistant_play.cpp

`voice_assistant_play.cpp` 文件使用科大讯飞的SDK实现语音识别功能。通过语音指令控制机器人，实现与用户的语音交互。

### 3. my_dynamixel 文件夹

`my_dynamixel` 文件夹包含适用于Ubuntu 20.04的Dynamixel机械臂控制包，具体包括如下内容：

- **src/all.py**：整合了物品抓取阶段的所有过程的代码。此文件调用其他模块，实现完整的抓取操作流程。
  
- **src/control_arm.py**：定义了控制机械臂进行物品抓取的函数和逻辑。根据物体的位置调整机械臂的姿态，实现抓取动作。

- **src/store_pose.py**：用于存储当前机器人的位置信息，可以通过语音指令进行触发，保存机器人的当前位置坐标。

- **src/get_pose.py**：用于获取先前存储的机器人的位置信息，通过语音指令调用，实现机器人的位置恢复。

- **src/pid.py**：实现PID控制算法，根据YOLO算法识别出的物体位置，实时调整机械臂的位置，确保抓取动作的精确性。
